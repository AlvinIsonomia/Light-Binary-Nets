{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "import torch\n",
    "import pdb\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def Binarize(tensor,quant_mode='det'):\n",
    "    if quant_mode=='det':\n",
    "        return tensor.sign()\n",
    "    else:\n",
    "        return tensor.add_(1).div_(2).add_(torch.rand(tensor.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)\n",
    "#clamp就是超过了range就直接咔嚓掉，截断函数\n",
    "\n",
    "\n",
    "\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HingeLoss,self).__init__()\n",
    "        self.margin=1.0\n",
    "\n",
    "    def hinge_loss(self,input,target):\n",
    "            #import pdb; pdb.set_trace()\n",
    "            output=self.margin-input.mul(target)\n",
    "            output[output.le(0)]=0 #less or equal!\n",
    "            return output.mean()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.hinge_loss(input,target)\n",
    "\n",
    "class SqrtHingeLossFunction(Function):\n",
    "    def __init__(self):\n",
    "        super(SqrtHingeLossFunction,self).__init__()\n",
    "        self.margin=1.0\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        output=self.margin-input.mul(target)\n",
    "        output[output.le(0)]=0\n",
    "        self.save_for_backward(input, target)\n",
    "        loss=output.mul(output).sum(0).sum(1).div(target.numel())\n",
    "        return loss\n",
    "\n",
    "    def backward(self,grad_output):\n",
    "       input, target = self.saved_tensors\n",
    "       output=self.margin-input.mul(target)\n",
    "       output[output.le(0)]=0\n",
    "       import pdb; pdb.set_trace()\n",
    "       grad_output.resize_as_(input).copy_(target).mul_(-2).mul_(output)\n",
    "       grad_output.mul_(output.ne(0).float())\n",
    "       grad_output.div_(input.numel())\n",
    "       return grad_output,grad_output\n",
    "\n",
    "def Quantize(tensor,quant_mode='det',  params=None, numBits=8):\n",
    "    tensor.clamp_(-2**(numBits-1),2**(numBits-1))\n",
    "    if quant_mode=='det':\n",
    "        tensor=tensor.mul(2**(numBits-1)).round().div(2**(numBits-1))\n",
    "    else:\n",
    "        tensor=tensor.mul(2**(numBits-1)).round().add(torch.rand(tensor.size()).add(-0.5)).div(2**(numBits-1))\n",
    "        quant_fixed(tensor, params)\n",
    "    return tensor\n",
    "\n",
    "import torch.nn._functions as tnnf\n",
    "\n",
    "\n",
    "class BinarizeLinear(nn.Linear):\n",
    "\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BinarizeLinear, self).__init__(*kargs, **kwargs)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        if input.size(1) != 784:\n",
    "            input.data=Binarize(input.data)\n",
    "        if not hasattr(self.weight,'org'):\n",
    "            self.weight.org=self.weight.data.clone()\n",
    "        self.weight.data=Binarize(self.weight.org)\n",
    "        out = nn.functional.linear(input, self.weight)\n",
    "        if not self.bias is None:\n",
    "            self.bias.org=self.bias.data.clone()\n",
    "            out += self.bias.view(1, -1).expand_as(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class BinarizeConv2d(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BinarizeConv2d, self).__init__(*kargs, **kwargs)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.size(1) != 3:\n",
    "            input.data = Binarize(input.data)\n",
    "        if not hasattr(self.weight,'org'):\n",
    "            self.weight.org=self.weight.data.clone()\n",
    "        self.weight.data=Binarize(self.weight.org)\n",
    "\n",
    "        out = nn.functional.conv2d(input, self.weight, None, self.stride,\n",
    "                                   self.padding, self.dilation, self.groups)\n",
    "\n",
    "        if not self.bias is None:\n",
    "            self.bias.org=self.bias.data.clone()\n",
    "            out += self.bias.view(1, -1, 1, 1).expand_as(out)\n",
    "\n",
    "        return out\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.backends import cudnn\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super(MobileNet, self).__init__()\n",
    "\n",
    "        def conv_bn(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "#                 BinarizeConv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.Hardtanh(inplace=True),\n",
    "            )\n",
    "\n",
    "        def conv_dw(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "#                 BinarizeConv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "                nn.BatchNorm2d(inp),\n",
    "                nn.Hardtanh(inplace=True),\n",
    "    \n",
    "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "#                 BinarizeConv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.Hardtanh(inplace=True),\n",
    "            )\n",
    "        def bconv_dw(inp,oup,stride):\n",
    "            return nn.Sequential(\n",
    "                BinarizeConv2d(inp,inp,3,stride,1,groups = inp ,bias = True),\n",
    "                nn.BatchNorm2d(inp),\n",
    "                nn.Hardtanh(inplace = True),\n",
    "                BinarizeConv2d(inp,oup,1,1,0,bias = True),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.Hardtanh(inplace=True),\n",
    "            )\n",
    "        \n",
    "        self.ratioInfl = 1\n",
    "        self.model = nn.Sequential(\n",
    "            conv_bn(  3,  32*self.ratioInfl, 2), \n",
    "            conv_dw( 32*self.ratioInfl,  64*self.ratioInfl, 1),\n",
    "            conv_dw( 64*self.ratioInfl, 128*self.ratioInfl, 2),\n",
    "            conv_dw(128*self.ratioInfl, 128*self.ratioInfl, 1),\n",
    "            conv_dw(128*self.ratioInfl, 256*self.ratioInfl, 2),\n",
    "            bconv_dw(256*self.ratioInfl, 256*self.ratioInfl, 1),\n",
    "            bconv_dw(256*self.ratioInfl, 512*self.ratioInfl, 2),\n",
    "            bconv_dw(512*self.ratioInfl, 512*self.ratioInfl, 1),\n",
    "            conv_dw(512*self.ratioInfl, 512*self.ratioInfl, 1),\n",
    "            conv_dw(512*self.ratioInfl, 512*self.ratioInfl, 1),\n",
    "            conv_dw(512*self.ratioInfl, 512*self.ratioInfl, 1),\n",
    "            conv_dw(512*self.ratioInfl, 512*self.ratioInfl, 1),\n",
    "            bconv_dw(512*self.ratioInfl, 1024*self.ratioInfl, 2),\n",
    "            bconv_dw(1024*self.ratioInfl, 1024, 1),\n",
    "            nn.AvgPool2d(7),\n",
    "        )\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        self.regime = {\n",
    "            0: {'optimizer': 'Adam', 'lr': 5e-3},\n",
    "            25: {'lr': 1e-2},\n",
    "            50: {'lr': 5e-4},\n",
    "            75: {'lr': 1e-5},\n",
    "            100: {'lr': 1e-6},\n",
    "            150: {'lr': 1e-7},\n",
    "            200: {'lr': 1e-6},\n",
    "            250: {'lr': 1e-7},\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def v12_binary(**kwargs):\n",
    "    num_classes = getattr(kwargs,'num_classes', 10)\n",
    "    return MobileNet(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该层的结构：[32, 3, 3, 3]\n",
      "该层参数和：864\n",
      "该层的结构：[32]\n",
      "该层参数和：32\n",
      "该层的结构：[32]\n",
      "该层参数和：32\n",
      "该层的结构：[32, 1, 3, 3]\n",
      "该层参数和：288\n",
      "该层的结构：[32]\n",
      "该层参数和：32\n",
      "该层的结构：[32]\n",
      "该层参数和：32\n",
      "该层的结构：[64, 32, 1, 1]\n",
      "该层参数和：2048\n",
      "该层的结构：[64]\n",
      "该层参数和：64\n",
      "该层的结构：[64]\n",
      "该层参数和：64\n",
      "该层的结构：[64, 1, 3, 3]\n",
      "该层参数和：576\n",
      "该层的结构：[64]\n",
      "该层参数和：64\n",
      "该层的结构：[64]\n",
      "该层参数和：64\n",
      "该层的结构：[128, 64, 1, 1]\n",
      "该层参数和：8192\n",
      "该层的结构：[128]\n",
      "该层参数和：128\n",
      "该层的结构：[128]\n",
      "该层参数和：128\n",
      "该层的结构：[128, 1, 3, 3]\n",
      "该层参数和：1152\n",
      "该层的结构：[128]\n",
      "该层参数和：128\n",
      "该层的结构：[128]\n",
      "该层参数和：128\n",
      "该层的结构：[128, 128, 1, 1]\n",
      "该层参数和：16384\n",
      "该层的结构：[128]\n",
      "该层参数和：128\n",
      "该层的结构：[128]\n",
      "该层参数和：128\n",
      "该层的结构：[128, 1, 3, 3]\n",
      "该层参数和：1152\n",
      "该层的结构：[128]\n",
      "该层参数和：128\n",
      "该层的结构：[128]\n",
      "该层参数和：128\n",
      "该层的结构：[256, 128, 1, 1]\n",
      "该层参数和：32768\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[256, 1, 3, 3]\n",
      "该层参数和：2304\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[256, 256, 1, 1]\n",
      "该层参数和：65536\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[256, 1, 3, 3]\n",
      "该层参数和：2304\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[256]\n",
      "该层参数和：256\n",
      "该层的结构：[512, 256, 1, 1]\n",
      "该层参数和：131072\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512, 1, 3, 3]\n",
      "该层参数和：4608\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512, 512, 1, 1]\n",
      "该层参数和：262144\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512, 1, 3, 3]\n",
      "该层参数和：4608\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512, 512, 1, 1]\n",
      "该层参数和：262144\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512, 1, 3, 3]\n",
      "该层参数和：4608\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512, 512, 1, 1]\n",
      "该层参数和：262144\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512, 1, 3, 3]\n",
      "该层参数和：4608\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512, 512, 1, 1]\n",
      "该层参数和：262144\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512, 1, 3, 3]\n",
      "该层参数和：4608\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512, 512, 1, 1]\n",
      "该层参数和：262144\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512, 1, 3, 3]\n",
      "该层参数和：4608\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[512]\n",
      "该层参数和：512\n",
      "该层的结构：[1024, 512, 1, 1]\n",
      "该层参数和：524288\n",
      "该层的结构：[1024]\n",
      "该层参数和：1024\n",
      "该层的结构：[1024]\n",
      "该层参数和：1024\n",
      "该层的结构：[1024]\n",
      "该层参数和：1024\n",
      "该层的结构：[1024, 1, 3, 3]\n",
      "该层参数和：9216\n",
      "该层的结构：[1024]\n",
      "该层参数和：1024\n",
      "该层的结构：[1024]\n",
      "该层参数和：1024\n",
      "该层的结构：[1024]\n",
      "该层参数和：1024\n",
      "该层的结构：[1024, 1024, 1, 1]\n",
      "该层参数和：1048576\n",
      "该层的结构：[1024]\n",
      "该层参数和：1024\n",
      "该层的结构：[1024]\n",
      "该层参数和：1024\n",
      "该层的结构：[1024]\n",
      "该层参数和：1024\n",
      "该层的结构：[10, 1024]\n",
      "该层参数和：10240\n",
      "该层的结构：[10]\n",
      "该层参数和：10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of MobileNet(\n",
       "  (model): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "      (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "      (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "      (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "      (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BinarizeConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "      (3): BinarizeConv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BinarizeConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "      (3): BinarizeConv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BinarizeConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "      (3): BinarizeConv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "    )\n",
       "    (11): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "    )\n",
       "    (12): Sequential(\n",
       "      (0): BinarizeConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "      (3): BinarizeConv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "    )\n",
       "    (13): Sequential(\n",
       "      (0): BinarizeConv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "      (3): BinarizeConv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Hardtanh(min_val=-1.0, max_val=1.0, inplace)\n",
       "    )\n",
       "    (14): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MobileNet(10)\n",
    "params = list(net.parameters())\n",
    "k = 0\n",
    "suma = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "    suma += k\n",
    "net.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference speed in gpus\n",
      "  resnet18 : 0.004733\n",
      "   alexnet : 0.001034\n",
      "     vgg16 : 0.001882\n",
      "squeezenet : 0.003498\n",
      " mobilenet : 0.006373\n",
      "mobilenet_binary : 0.006862\n",
      "inference speed in cpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/liuchang/anaconda3/envs/py3/lib/python3.7/site-packages/ipykernel_launcher.py:58: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/data/liuchang/anaconda3/envs/py3/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  resnet18 : 0.044572\n",
      "   alexnet : 0.019290\n",
      "     vgg16 : 0.125436\n",
      "squeezenet : 0.059603\n",
      " mobilenet : 0.036250\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1b6ebbfed005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mspeed_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqueezenet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'squeezenet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mspeed_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmobilenet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mobilenet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mspeed_cpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mobilenet_binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-1b6ebbfed005>\u001b[0m in \u001b[0;36mspeed_cpu\u001b[0;34m(model, name)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/liuchang/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-391ade85f70a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/liuchang/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/liuchang/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/liuchang/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/liuchang/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/liuchang/anaconda3/envs/py3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-391ade85f70a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         out = nn.functional.conv2d(input, self.weight, None, self.stride,\n\u001b[0;32m--> 104\u001b[0;31m                                    self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileNet, self).__init__()\n",
    "\n",
    "        def conv_bn(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        def conv_dw(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "                nn.BatchNorm2d(inp),\n",
    "                nn.ReLU(inplace=True),\n",
    "    \n",
    "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            conv_bn(  3,  32, 2), \n",
    "            conv_dw( 32,  64, 1),\n",
    "            conv_dw( 64, 128, 2),\n",
    "            conv_dw(128, 128, 1),\n",
    "            conv_dw(128, 256, 2),\n",
    "            conv_dw(256, 256, 1),\n",
    "            conv_dw(256, 512, 2),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 1024, 2),\n",
    "            conv_dw(1024, 1024, 1),\n",
    "            nn.AvgPool2d(7),\n",
    "        )\n",
    "        self.fc = nn.Linear(1024, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def speed(model, name):\n",
    "    t0 = time.time()\n",
    "    input = torch.rand(1,3,224,224).cuda()\n",
    "    input = Variable(input, volatile = True)\n",
    "    t1 = time.time()\n",
    "\n",
    "    model(input)\n",
    "    t2 = time.time()\n",
    "\n",
    "    model(input)\n",
    "    t3 = time.time()\n",
    "    \n",
    "    print('%10s : %f' % (name, t3 - t2))\n",
    "def speed_cpu(model, name):\n",
    "    t0 = time.time()\n",
    "    input = torch.rand(1,3,224,224)\n",
    "    input = Variable(input, volatile = True)\n",
    "    t1 = time.time()\n",
    "\n",
    "    model(input)\n",
    "    t2 = time.time()\n",
    "\n",
    "    model(input)\n",
    "    t3 = time.time()\n",
    "    \n",
    "    print('%10s : %f' % (name, t3 - t2))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #cudnn.benchmark = True # This will make network slow ??\n",
    "    resnet18 = models.resnet18().cuda()\n",
    "    alexnet = models.alexnet().cuda()\n",
    "    vgg16 = models.vgg16().cuda()\n",
    "    squeezenet = models.squeezenet1_0().cuda()\n",
    "    mobilenet = MobileNet().cuda()\n",
    "    net = net.cuda()\n",
    "    print('inference speed in gpus')\n",
    "    speed(resnet18, 'resnet18')\n",
    "    speed(alexnet, 'alexnet')\n",
    "    speed(vgg16, 'vgg16')\n",
    "    speed(squeezenet, 'squeezenet')\n",
    "    speed(mobilenet, 'mobilenet')\n",
    "    speed(net,'mobilenet_binary')\n",
    "    \n",
    "    print('inference speed in cpus')\n",
    "    resnet18 = resnet18.cpu()\n",
    "    alexnet = alexnet.cpu()\n",
    "    vgg16 = vgg16.cpu()\n",
    "    squeezenet = squeezenet.cpu()\n",
    "    mobilenet = mobilenet.cpu()\n",
    "    net = net.cpu()\n",
    "    speed_cpu(resnet18, 'resnet18')\n",
    "    speed_cpu(alexnet, 'alexnet')\n",
    "    speed_cpu(vgg16, 'vgg16')\n",
    "    speed_cpu(squeezenet, 'squeezenet')\n",
    "    speed_cpu(mobilenet, 'mobilenet')\n",
    "    speed_cpu(net,'mobilenet_binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
