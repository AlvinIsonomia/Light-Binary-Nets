{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for this version, Datatype must be float tensor,modified would come as soon as possible\n",
      "saving to ./results/2019-10-30_09-36-51\n",
      "creating model vgg_cifar10_binary\n",
      "created model with configuration: {'input_size': None, 'dataset': 'cifar10'}\n",
      "number of parameters: 36461450\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "training regime: {0: {'optimizer': 'Adam', 'betas': (0.9, 0.999), 'lr': 0.005}, 40: {'lr': 0.001}, 80: {'lr': 0.0005}, 100: {'lr': 0.0001}, 120: {'lr': 5e-05}, 140: {'lr': 1e-05}}\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "\n",
      " Epoch: 1\tTraining Loss 1.8327 \tTraining Prec@1 34.002 \tTraining Prec@5 84.108 \tValidation Loss 1.6560 \tValidation Prec@1 39.980 \tValidation Prec@5 90.390 \n",
      "\n",
      "\n",
      " Epoch: 2\tTraining Loss 1.4593 \tTraining Prec@1 50.528 \tTraining Prec@5 92.218 \tValidation Loss 1.2821 \tValidation Prec@1 59.080 \tValidation Prec@5 94.440 \n",
      "\n",
      "\n",
      " Epoch: 3\tTraining Loss 1.2037 \tTraining Prec@1 61.574 \tTraining Prec@5 95.006 \tValidation Loss 1.2468 \tValidation Prec@1 59.610 \tValidation Prec@5 95.350 \n",
      "\n",
      "\n",
      " Epoch: 4\tTraining Loss 1.0748 \tTraining Prec@1 66.738 \tTraining Prec@5 96.384 \tValidation Loss 1.1642 \tValidation Prec@1 63.680 \tValidation Prec@5 95.340 \n",
      "\n",
      "\n",
      " Epoch: 5\tTraining Loss 0.9655 \tTraining Prec@1 71.566 \tTraining Prec@5 97.254 \tValidation Loss 1.2916 \tValidation Prec@1 58.690 \tValidation Prec@5 94.110 \n",
      "\n",
      "\n",
      " Epoch: 6\tTraining Loss 0.9036 \tTraining Prec@1 74.332 \tTraining Prec@5 97.668 \tValidation Loss 1.1103 \tValidation Prec@1 65.410 \tValidation Prec@5 96.890 \n",
      "\n",
      "\n",
      " Epoch: 7\tTraining Loss 0.8509 \tTraining Prec@1 76.404 \tTraining Prec@5 97.940 \tValidation Loss 0.9972 \tValidation Prec@1 70.350 \tValidation Prec@5 96.950 \n",
      "\n",
      "\n",
      " Epoch: 8\tTraining Loss 0.8088 \tTraining Prec@1 78.326 \tTraining Prec@5 98.354 \tValidation Loss 1.0757 \tValidation Prec@1 66.890 \tValidation Prec@5 97.260 \n",
      "\n",
      "\n",
      " Epoch: 9\tTraining Loss 0.7756 \tTraining Prec@1 79.758 \tTraining Prec@5 98.438 \tValidation Loss 0.9833 \tValidation Prec@1 71.440 \tValidation Prec@5 95.950 \n",
      "\n",
      "\n",
      " Epoch: 10\tTraining Loss 0.7447 \tTraining Prec@1 81.182 \tTraining Prec@5 98.532 \tValidation Loss 0.8734 \tValidation Prec@1 75.670 \tValidation Prec@5 97.240 \n",
      "\n",
      "\n",
      " Epoch: 11\tTraining Loss 0.7182 \tTraining Prec@1 82.156 \tTraining Prec@5 98.642 \tValidation Loss 0.8758 \tValidation Prec@1 75.250 \tValidation Prec@5 97.720 \n",
      "\n",
      "\n",
      " Epoch: 12\tTraining Loss 0.7014 \tTraining Prec@1 82.968 \tTraining Prec@5 98.758 \tValidation Loss 0.9072 \tValidation Prec@1 74.600 \tValidation Prec@5 96.670 \n",
      "\n",
      "\n",
      " Epoch: 13\tTraining Loss 0.6776 \tTraining Prec@1 83.950 \tTraining Prec@5 98.910 \tValidation Loss 0.8813 \tValidation Prec@1 75.320 \tValidation Prec@5 97.280 \n",
      "\n",
      "\n",
      " Epoch: 14\tTraining Loss 0.6603 \tTraining Prec@1 84.626 \tTraining Prec@5 98.936 \tValidation Loss 0.8274 \tValidation Prec@1 78.560 \tValidation Prec@5 97.530 \n",
      "\n",
      "\n",
      " Epoch: 15\tTraining Loss 0.6472 \tTraining Prec@1 85.344 \tTraining Prec@5 99.026 \tValidation Loss 0.7844 \tValidation Prec@1 79.730 \tValidation Prec@5 98.380 \n",
      "\n",
      "\n",
      " Epoch: 16\tTraining Loss 0.6313 \tTraining Prec@1 85.902 \tTraining Prec@5 99.076 \tValidation Loss 0.7421 \tValidation Prec@1 81.300 \tValidation Prec@5 98.460 \n",
      "\n",
      "\n",
      " Epoch: 17\tTraining Loss 0.6244 \tTraining Prec@1 86.182 \tTraining Prec@5 99.182 \tValidation Loss 0.8219 \tValidation Prec@1 78.360 \tValidation Prec@5 97.620 \n",
      "\n",
      "\n",
      " Epoch: 18\tTraining Loss 0.6112 \tTraining Prec@1 86.854 \tTraining Prec@5 99.152 \tValidation Loss 0.8044 \tValidation Prec@1 78.490 \tValidation Prec@5 98.360 \n",
      "\n",
      "\n",
      " Epoch: 19\tTraining Loss 0.5978 \tTraining Prec@1 87.364 \tTraining Prec@5 99.194 \tValidation Loss 0.8842 \tValidation Prec@1 75.460 \tValidation Prec@5 97.420 \n",
      "\n",
      "\n",
      " Epoch: 20\tTraining Loss 0.5929 \tTraining Prec@1 87.624 \tTraining Prec@5 99.210 \tValidation Loss 0.7321 \tValidation Prec@1 81.670 \tValidation Prec@5 98.310 \n",
      "\n",
      "\n",
      " Epoch: 21\tTraining Loss 0.5791 \tTraining Prec@1 88.164 \tTraining Prec@5 99.342 \tValidation Loss 0.8655 \tValidation Prec@1 76.790 \tValidation Prec@5 97.650 \n",
      "\n",
      "\n",
      " Epoch: 22\tTraining Loss 0.5721 \tTraining Prec@1 88.564 \tTraining Prec@5 99.288 \tValidation Loss 0.7589 \tValidation Prec@1 80.820 \tValidation Prec@5 98.240 \n",
      "\n",
      "\n",
      " Epoch: 23\tTraining Loss 0.5617 \tTraining Prec@1 89.028 \tTraining Prec@5 99.410 \tValidation Loss 0.7101 \tValidation Prec@1 82.490 \tValidation Prec@5 98.260 \n",
      "\n",
      "\n",
      " Epoch: 24\tTraining Loss 0.5509 \tTraining Prec@1 89.496 \tTraining Prec@5 99.438 \tValidation Loss 0.8389 \tValidation Prec@1 78.130 \tValidation Prec@5 97.260 \n",
      "\n",
      "\n",
      " Epoch: 25\tTraining Loss 0.5447 \tTraining Prec@1 89.756 \tTraining Prec@5 99.428 \tValidation Loss 0.7369 \tValidation Prec@1 82.010 \tValidation Prec@5 97.450 \n",
      "\n",
      "\n",
      " Epoch: 26\tTraining Loss 0.5380 \tTraining Prec@1 90.046 \tTraining Prec@5 99.448 \tValidation Loss 0.8974 \tValidation Prec@1 75.370 \tValidation Prec@5 97.080 \n",
      "\n",
      "\n",
      " Epoch: 27\tTraining Loss 0.5335 \tTraining Prec@1 90.214 \tTraining Prec@5 99.530 \tValidation Loss 0.7017 \tValidation Prec@1 83.050 \tValidation Prec@5 98.590 \n",
      "\n",
      "\n",
      " Epoch: 28\tTraining Loss 0.5251 \tTraining Prec@1 90.628 \tTraining Prec@5 99.470 \tValidation Loss 0.8199 \tValidation Prec@1 78.690 \tValidation Prec@5 96.670 \n",
      "\n",
      "\n",
      " Epoch: 29\tTraining Loss 0.5185 \tTraining Prec@1 90.902 \tTraining Prec@5 99.504 \tValidation Loss 0.7725 \tValidation Prec@1 80.940 \tValidation Prec@5 98.140 \n",
      "\n",
      "\n",
      " Epoch: 30\tTraining Loss 0.5149 \tTraining Prec@1 91.080 \tTraining Prec@5 99.560 \tValidation Loss 0.8071 \tValidation Prec@1 79.330 \tValidation Prec@5 97.350 \n",
      "\n",
      "\n",
      " Epoch: 31\tTraining Loss 0.5085 \tTraining Prec@1 91.416 \tTraining Prec@5 99.522 \tValidation Loss 0.6808 \tValidation Prec@1 83.650 \tValidation Prec@5 98.410 \n",
      "\n",
      "\n",
      " Epoch: 32\tTraining Loss 0.5049 \tTraining Prec@1 91.408 \tTraining Prec@5 99.600 \tValidation Loss 0.7413 \tValidation Prec@1 81.870 \tValidation Prec@5 97.470 \n",
      "\n",
      "\n",
      " Epoch: 33\tTraining Loss 0.4995 \tTraining Prec@1 91.816 \tTraining Prec@5 99.562 \tValidation Loss 0.7226 \tValidation Prec@1 82.510 \tValidation Prec@5 98.010 \n",
      "\n",
      "\n",
      " Epoch: 34\tTraining Loss 0.4942 \tTraining Prec@1 91.942 \tTraining Prec@5 99.654 \tValidation Loss 0.6826 \tValidation Prec@1 83.670 \tValidation Prec@5 98.410 \n",
      "\n",
      "\n",
      " Epoch: 35\tTraining Loss 0.4889 \tTraining Prec@1 92.176 \tTraining Prec@5 99.606 \tValidation Loss 0.6440 \tValidation Prec@1 85.670 \tValidation Prec@5 98.660 \n",
      "\n",
      "\n",
      " Epoch: 36\tTraining Loss 0.4830 \tTraining Prec@1 92.348 \tTraining Prec@5 99.626 \tValidation Loss 0.7005 \tValidation Prec@1 83.500 \tValidation Prec@5 98.440 \n",
      "\n",
      "\n",
      " Epoch: 37\tTraining Loss 0.4848 \tTraining Prec@1 92.404 \tTraining Prec@5 99.676 \tValidation Loss 0.7693 \tValidation Prec@1 80.860 \tValidation Prec@5 97.320 \n",
      "\n",
      "\n",
      " Epoch: 38\tTraining Loss 0.4783 \tTraining Prec@1 92.612 \tTraining Prec@5 99.662 \tValidation Loss 0.7268 \tValidation Prec@1 82.480 \tValidation Prec@5 97.690 \n",
      "\n",
      "\n",
      " Epoch: 39\tTraining Loss 0.4762 \tTraining Prec@1 92.668 \tTraining Prec@5 99.676 \tValidation Loss 0.7450 \tValidation Prec@1 81.730 \tValidation Prec@5 97.900 \n",
      "\n",
      "\n",
      " Epoch: 40\tTraining Loss 0.4752 \tTraining Prec@1 92.752 \tTraining Prec@5 99.694 \tValidation Loss 0.6820 \tValidation Prec@1 84.190 \tValidation Prec@5 98.320 \n",
      "\n",
      "\n",
      " Epoch: 41\tTraining Loss 0.4196 \tTraining Prec@1 95.148 \tTraining Prec@5 99.814 \tValidation Loss 0.5613 \tValidation Prec@1 88.650 \tValidation Prec@5 98.910 \n",
      "\n",
      "\n",
      " Epoch: 42\tTraining Loss 0.4149 \tTraining Prec@1 95.522 \tTraining Prec@5 99.818 \tValidation Loss 0.5598 \tValidation Prec@1 89.080 \tValidation Prec@5 99.170 \n",
      "\n",
      "\n",
      " Epoch: 43\tTraining Loss 0.4069 \tTraining Prec@1 95.806 \tTraining Prec@5 99.826 \tValidation Loss 0.5780 \tValidation Prec@1 88.310 \tValidation Prec@5 98.820 \n",
      "\n",
      "\n",
      " Epoch: 44\tTraining Loss 0.4059 \tTraining Prec@1 95.942 \tTraining Prec@5 99.834 \tValidation Loss 0.5707 \tValidation Prec@1 88.550 \tValidation Prec@5 98.770 \n",
      "\n",
      "\n",
      " Epoch: 45\tTraining Loss 0.4017 \tTraining Prec@1 96.056 \tTraining Prec@5 99.868 \tValidation Loss 0.5581 \tValidation Prec@1 88.880 \tValidation Prec@5 98.700 \n",
      "\n",
      "\n",
      " Epoch: 46\tTraining Loss 0.3991 \tTraining Prec@1 96.134 \tTraining Prec@5 99.874 \tValidation Loss 0.5624 \tValidation Prec@1 89.110 \tValidation Prec@5 98.950 \n",
      "\n",
      "\n",
      " Epoch: 47\tTraining Loss 0.3960 \tTraining Prec@1 96.254 \tTraining Prec@5 99.856 \tValidation Loss 0.5637 \tValidation Prec@1 88.660 \tValidation Prec@5 98.750 \n",
      "\n",
      "\n",
      " Epoch: 48\tTraining Loss 0.3942 \tTraining Prec@1 96.310 \tTraining Prec@5 99.886 \tValidation Loss 0.6074 \tValidation Prec@1 87.790 \tValidation Prec@5 98.420 \n",
      "\n",
      "\n",
      " Epoch: 49\tTraining Loss 0.3938 \tTraining Prec@1 96.362 \tTraining Prec@5 99.894 \tValidation Loss 0.5793 \tValidation Prec@1 88.700 \tValidation Prec@5 98.500 \n",
      "\n",
      "\n",
      " Epoch: 50\tTraining Loss 0.3934 \tTraining Prec@1 96.362 \tTraining Prec@5 99.866 \tValidation Loss 0.5770 \tValidation Prec@1 88.730 \tValidation Prec@5 98.840 \n",
      "\n",
      "\n",
      " Epoch: 51\tTraining Loss 0.3925 \tTraining Prec@1 96.442 \tTraining Prec@5 99.854 \tValidation Loss 0.5987 \tValidation Prec@1 87.470 \tValidation Prec@5 98.850 \n",
      "\n",
      "\n",
      " Epoch: 52\tTraining Loss 0.3882 \tTraining Prec@1 96.636 \tTraining Prec@5 99.904 \tValidation Loss 0.5725 \tValidation Prec@1 88.830 \tValidation Prec@5 98.740 \n",
      "\n",
      "\n",
      " Epoch: 53\tTraining Loss 0.3863 \tTraining Prec@1 96.634 \tTraining Prec@5 99.896 \tValidation Loss 0.5596 \tValidation Prec@1 89.200 \tValidation Prec@5 98.920 \n",
      "\n",
      "\n",
      " Epoch: 54\tTraining Loss 0.3866 \tTraining Prec@1 96.630 \tTraining Prec@5 99.920 \tValidation Loss 0.5494 \tValidation Prec@1 89.630 \tValidation Prec@5 98.910 \n",
      "\n",
      "\n",
      " Epoch: 55\tTraining Loss 0.3863 \tTraining Prec@1 96.620 \tTraining Prec@5 99.890 \tValidation Loss 0.5707 \tValidation Prec@1 89.190 \tValidation Prec@5 98.600 \n",
      "\n",
      "\n",
      " Epoch: 56\tTraining Loss 0.3829 \tTraining Prec@1 96.908 \tTraining Prec@5 99.880 \tValidation Loss 0.5582 \tValidation Prec@1 89.320 \tValidation Prec@5 98.740 \n",
      "\n",
      "\n",
      " Epoch: 57\tTraining Loss 0.3817 \tTraining Prec@1 96.920 \tTraining Prec@5 99.900 \tValidation Loss 0.5655 \tValidation Prec@1 89.250 \tValidation Prec@5 98.780 \n",
      "\n",
      "\n",
      " Epoch: 58\tTraining Loss 0.3797 \tTraining Prec@1 97.052 \tTraining Prec@5 99.912 \tValidation Loss 0.5545 \tValidation Prec@1 89.520 \tValidation Prec@5 98.720 \n",
      "\n",
      "\n",
      " Epoch: 59\tTraining Loss 0.3791 \tTraining Prec@1 96.972 \tTraining Prec@5 99.918 \tValidation Loss 0.5574 \tValidation Prec@1 89.160 \tValidation Prec@5 98.780 \n",
      "\n",
      "\n",
      " Epoch: 60\tTraining Loss 0.3783 \tTraining Prec@1 97.048 \tTraining Prec@5 99.942 \tValidation Loss 0.5651 \tValidation Prec@1 89.260 \tValidation Prec@5 98.860 \n",
      "\n",
      "\n",
      " Epoch: 61\tTraining Loss 0.3782 \tTraining Prec@1 97.036 \tTraining Prec@5 99.900 \tValidation Loss 0.5619 \tValidation Prec@1 89.110 \tValidation Prec@5 98.940 \n",
      "\n",
      "\n",
      " Epoch: 62\tTraining Loss 0.3739 \tTraining Prec@1 97.244 \tTraining Prec@5 99.922 \tValidation Loss 0.5735 \tValidation Prec@1 88.710 \tValidation Prec@5 98.850 \n",
      "\n",
      "\n",
      " Epoch: 63\tTraining Loss 0.3749 \tTraining Prec@1 97.202 \tTraining Prec@5 99.914 \tValidation Loss 0.5742 \tValidation Prec@1 88.520 \tValidation Prec@5 98.810 \n",
      "\n",
      "\n",
      " Epoch: 64\tTraining Loss 0.3715 \tTraining Prec@1 97.332 \tTraining Prec@5 99.930 \tValidation Loss 0.5503 \tValidation Prec@1 89.600 \tValidation Prec@5 98.740 \n",
      "\n",
      "\n",
      " Epoch: 65\tTraining Loss 0.3760 \tTraining Prec@1 97.170 \tTraining Prec@5 99.908 \tValidation Loss 0.5786 \tValidation Prec@1 88.410 \tValidation Prec@5 98.520 \n",
      "\n",
      "\n",
      " Epoch: 66\tTraining Loss 0.3747 \tTraining Prec@1 97.244 \tTraining Prec@5 99.908 \tValidation Loss 0.5609 \tValidation Prec@1 89.350 \tValidation Prec@5 98.910 \n",
      "\n",
      "\n",
      " Epoch: 67\tTraining Loss 0.3731 \tTraining Prec@1 97.314 \tTraining Prec@5 99.940 \tValidation Loss 0.5735 \tValidation Prec@1 88.980 \tValidation Prec@5 98.240 \n",
      "\n",
      "\n",
      " Epoch: 68\tTraining Loss 0.3712 \tTraining Prec@1 97.396 \tTraining Prec@5 99.914 \tValidation Loss 0.5516 \tValidation Prec@1 89.340 \tValidation Prec@5 98.910 \n",
      "\n",
      "\n",
      " Epoch: 69\tTraining Loss 0.3688 \tTraining Prec@1 97.436 \tTraining Prec@5 99.958 \tValidation Loss 0.5916 \tValidation Prec@1 88.580 \tValidation Prec@5 98.110 \n",
      "\n",
      "\n",
      " Epoch: 70\tTraining Loss 0.3667 \tTraining Prec@1 97.504 \tTraining Prec@5 99.946 \tValidation Loss 0.5570 \tValidation Prec@1 89.480 \tValidation Prec@5 98.740 \n",
      "\n",
      "\n",
      " Epoch: 71\tTraining Loss 0.3697 \tTraining Prec@1 97.522 \tTraining Prec@5 99.954 \tValidation Loss 0.5613 \tValidation Prec@1 89.060 \tValidation Prec@5 98.580 \n",
      "\n",
      "\n",
      " Epoch: 72\tTraining Loss 0.3702 \tTraining Prec@1 97.424 \tTraining Prec@5 99.930 \tValidation Loss 0.5784 \tValidation Prec@1 88.650 \tValidation Prec@5 98.570 \n",
      "\n",
      "\n",
      " Epoch: 73\tTraining Loss 0.3684 \tTraining Prec@1 97.476 \tTraining Prec@5 99.942 \tValidation Loss 0.5511 \tValidation Prec@1 89.620 \tValidation Prec@5 98.700 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python main_binary.py --model vgg_cifar10_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ResNet\n",
    "\n",
    "17528 paramers \n",
    "\n",
    "Epoch55 91.438%\n",
    "- Binary ResNet\n",
    "\n",
    "175406 paramerter 10 times more paramerters!\n",
    "\n",
    "epoch2500 67.816%  can't converge\n",
    "\n",
    "- Vgg\n",
    "\n",
    "36461450\n",
    "epoch 41 89.620% \t\n",
    "\n",
    "- Our model\n",
    "\n",
    "6367370 89.005%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
